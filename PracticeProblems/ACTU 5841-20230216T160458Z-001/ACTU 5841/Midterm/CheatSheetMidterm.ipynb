{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9c002e",
   "metadata": {},
   "source": [
    "## Cheat-Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceab89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43ec9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data file\n",
    "df = pd.read_csv('data3.csv')\n",
    "#Filtering Data\n",
    "df = df[df['FACE']  >= 50000]\n",
    "\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b28725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box-Cox transformation\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method = 'box-cox', standardize = False)\n",
    "\n",
    "# reshaping the column FACE to be in the appropriate dimension for box-cox transformation\n",
    "FACEbc = np.asarray(df['FACE'])\n",
    "FACEbc = FACEbc.reshape(-1,1)\n",
    "\n",
    "# converting type of FACE to float instead of int\n",
    "# so that I can convert 0 to a very small number instead\n",
    "FACEbc = FACEbc.astype(float)\n",
    "FACEbc[FACEbc == 0] = 0.0000000001\n",
    "\n",
    "pt.fit(FACEbc)\n",
    "\n",
    "#Adding the transformed FACE values into a new column on dataframe\n",
    "FACEbc = pt.transform(FACEbc)\n",
    "df['FACEbc'] = FACEbc\n",
    "\n",
    "skew1 = round(df['FACE'].skew(),3)\n",
    "skew2 = round(df['FACEbc'].skew(),3)\n",
    "lambdas = pt.lambdas_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad1c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b71ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Centered and Normalizing\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def center_normalize_df (dataframe):\n",
    "    \n",
    "    for column in dataframe:\n",
    "        dataframe[column] = dataframe[column] - np.mean(dataframe[column])\n",
    "        dataframe[column] = dataframe[column] / norm(dataframe[column])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def center_normalize_array (array):\n",
    "    array = array - np.mean(array)\n",
    "    array = array / norm(array)\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185947d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b389f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "\n",
    "#Ploting two axes in one plot\n",
    "#Plotting Histogram\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(df['FACE'])\n",
    "ax1.set_title('histogram: skew = %s' % skew1)\n",
    "ax1.set_xlabel('original')\n",
    "\n",
    "ax2.hist(df['FACEbc'])\n",
    "ax2.set_title('histogram: skew = %s' % skew2)\n",
    "ax2.set_xlabel('optimal transformation: %s = %s' % (chr(0x03BB), round(lambdas,3)))\n",
    "\n",
    "#plot\n",
    "plt.show()\n",
    "\n",
    "#save to file\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"histogram.png\")\n",
    "\n",
    "\n",
    "\n",
    "#plotting line plot\n",
    "a = np.arange(0,updates-1,1)\n",
    "b = np.flip(results[4])\n",
    "\n",
    "plt.plot(a,b)\n",
    "plt.title('loss')\n",
    "plt.xlabel('update number')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plotting two line with scatter\n",
    "y_truth = 2 * np.sin(0.5*x - 3) +  0.1*x\n",
    "y_fitted = B[0,0] * np.sin(B[0,1]*x - B[0,2]) +  B[0,3]*x\n",
    "\n",
    "plt.scatter(x,y, label = \"observations\", alpha=0.5)\n",
    "plt.plot(x,y_truth, label = \"truth\")\n",
    "plt.plot(x,y_fitted, label = \"fitted\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cd9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ef8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy Variables for categorical variables\n",
    "#creating dummy variables for the categorical variable MARSTAT \n",
    "df_dc = pd.get_dummies(df, columns=['MARSTAT'])\n",
    "df_dc.head()\n",
    "\n",
    "#Adding ln(INCOME) as a new column on the dataframe \n",
    "df_dc['logINCOME'] = np.log(df_dc['INCOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3047b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "#Computing linear regression\n",
    "#Formula FACEbc = B0 + B1*EDUCATION + B2*NUMHH + B3*logINCOME + B4*MARSTAT_0 + B5*MARSTAT_2 + E\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = df_dc[['EDUCATION', 'NUMHH', 'logINCOME', 'MARSTAT_0', 'MARSTAT_2']]\n",
    "y = df_dc['FACEbc']\n",
    "reg = LinearRegression().fit(X, y)\n",
    "B = reg.coef_\n",
    "B = np.insert(B, 0, reg.intercept_, axis=0)\n",
    "print(B)\n",
    "\n",
    "#Computing standard error with n-1 degrees of freedom\n",
    "SE = np.std(y, ddof=1)\n",
    "print(SE)\n",
    "\n",
    "#Computing R2\n",
    "y_pred = reg.predict(X)\n",
    "from sklearn.metrics import r2_score\n",
    "R2 = r2_score(y, y_pred)\n",
    "print(R2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Conducting linear regression using OLS method\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = df_dc[['EDUCATION', 'NUMHH', 'logINCOME', 'MARSTAT_0', 'MARSTAT_2']]\n",
    "y = df_dc['FACEbc']\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "lm = sm.OLS(y, X2).fit()\n",
    "print(lm.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Prediction\n",
    "\n",
    "#Conducting prediction with the married man profile as described from the question\n",
    "X3 = np.array([['16', '4', np.log(120000), '0', '0']])\n",
    "Y3 = reg.predict(X3)\n",
    "\n",
    "#Computing upper and lower interval\n",
    "Y3_upper, Y3_lower = Y3+SE, Y3-SE\n",
    "#inverse transform after\n",
    "\n",
    "#Inversing the box-cox transformation for the prediction\n",
    "Y3 = Y3.reshape(-1,1)\n",
    "Y3 = pt.inverse_transform(Y3)\n",
    "\n",
    "#Inversing the box-cox transformation of the standard error\n",
    "SE = SE.reshape(-1,1)\n",
    "SE = pt.inverse_transform(SE)\n",
    "\n",
    "\n",
    "print('95%% likelihood that the true value is between $%.2f and $%.2f' % (Y3_lower, Y3_upper))\n",
    "print('True value: $%.2f' % Y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafe228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using lasso to eliminate some features\n",
    "X = df2[['x1', 'x2', 'x3', 'x4', 'x5']] #without response variable\n",
    "y = df2['y'] #response\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "#lambdas = np.linspace(0,0.1,101)  # if trying a specific range of lambda\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "#model = LassoCV(alphas = lambdas, cv=cv) # if spec range of lambda\n",
    "model = LassoCV(cv=cv)\n",
    "lassocv = model.fit(X, y)\n",
    "\n",
    "B_lasso = lassocv.coef_\n",
    "B_lasso = np.insert(B_lasso, 0, lassocv.intercept_, axis=0)\n",
    "\n",
    "\n",
    "print('The Coef are')\n",
    "print(B_lasso)\n",
    "#print('\\nlambda best is = %f' % lassocv.alpha_)\n",
    "\n",
    "# transformed data set after lasso\n",
    "df_lasso =  df2.loc[:, df2.columns != 'y'] * lassocv.coef_\n",
    "df_lasso = df_lasso.loc[:, (np.abs(df_lasso) > np.abs(0.00)).any(axis=0)]\n",
    "df_lasso = pd.concat([df_lasso, np.log(df2['y'])], axis = 1) #adding back the response variable\n",
    "df_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf67eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table\n",
    "iteration2 = np.arange(0,iteration+2, 1)\n",
    "df2 = pd.DataFrame(list(zip(iteration2, B_history, np.flip(loss_history))),\n",
    "                   columns = ['iteration', 'B_hat', 'loss'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "RMSE = np.sqrt(((y_hat - y) ** 2).mean())\n",
    "print('The estimated RMSE via LOOCV is\\nRMSE = %s' % (RMSE))\n",
    "\n",
    "#RMSE using sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y, y_hat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46890c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross Validation\n",
    "\n",
    "def cross_validation (df, func):\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "    kf_rmse = []\n",
    "    \n",
    "    for train, test in kf.split(df):\n",
    "        X_train = df.iloc[train].loc[:, df.columns != 'SalePrice']\n",
    "        X_train = X_train.squeeze()\n",
    "        X_test = df.iloc[test].loc[:, df.columns != 'SalePrice']\n",
    "        y_train = df.iloc[train].loc[:,'SalePrice']\n",
    "        y_test = df.iloc[test].loc[:,'SalePrice']\n",
    "        \n",
    "        reg = func.fit(X_train, y_train)\n",
    "        y_hat = reg.predict(X_test)\n",
    "        \n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        kf_rmse.append(mean_squared_error(y_test, y_hat, squared=False))\n",
    "        \n",
    "    kf_RMSE = (1/10) * np.sum(kf_rmse)\n",
    "        \n",
    "    return (kf_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0661971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 Fold CV\n",
    "x = df['x']\n",
    "y = df['y']\n",
    "B = np.array([0.5, 0.75])\n",
    "B = B.reshape(1,-1)\n",
    "#kf_df = pd.DataFrame(zip(x,y), columns = ['x', 'y'])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "kf_rmse = []\n",
    "\n",
    "for train, test in kf.split(df):\n",
    "    X_train = df.iloc[train].loc[:, df.columns != 'y']\n",
    "    X_train = X_train.squeeze()\n",
    "    X_test = df.iloc[test].loc[:, df.columns != 'y']\n",
    "    y_train = df.iloc[train].loc[:,'y']\n",
    "    y_test = df.iloc[test].loc[:,'y']\n",
    "\n",
    "    nr = newton_raphson(X_train,y_train,B,10000)\n",
    "    \n",
    "    y_hat = np.exp(nr[1][0,1] + nr[1][0,1]*X_test)\n",
    "    kf_rmse.append(mean_squared_error(y_test, y_hat, squared=False))\n",
    "\n",
    "kf_RMSE = (1/10) * np.sum(kf_rmse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "kf_rmse = []\n",
    "kf_r2 = []\n",
    "\n",
    "for train, test in kf.split(df2):\n",
    "    X_train = df2.iloc[train].loc[:, df2.columns != 'y']\n",
    "    X_train = X_train.squeeze()\n",
    "    X_test = df2.iloc[test].loc[:, df2.columns != 'y']\n",
    "    y_train = df2.iloc[train].loc[:,'y']\n",
    "    y_test = df2.iloc[test].loc[:,'y']\n",
    "\n",
    "    reg_kf = LinearRegression().fit(X_train, y_train)    \n",
    "    y_hat = reg_kf.predict(X_test)\n",
    "    \n",
    "    kf_rmse.append(mean_squared_error(y_test, y_hat, squared=False))\n",
    "    kf_r2.append(r2_score(y_test,y_hat))\n",
    "\n",
    "kf_RMSE = (1/10) * np.sum(kf_rmse)\n",
    "kf_R2 = (1/10) * np.sum(kf_r2)\n",
    "print(kf_RMSE)\n",
    "print(kf_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688d460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(B, y, x):\n",
    "    #depends on the dimmension of B\n",
    "    \n",
    "    B0 = B[0,0] \n",
    "    B1 = B[0,1]\n",
    "    \n",
    "    f0 = -2*np.exp(B0+B1*x)*(y-np.exp(B0+B1*x)) # dR/dB_0\n",
    "    f1 = -2*np.exp(B1*x+B0)*x*(y-np.exp(B0+B1*x)) # dR/dB_1\n",
    "    \n",
    "    return np.array([np.sum(f0), np.sum(f1)])\n",
    "\n",
    "def calc_Jacobian(B, y, x):\n",
    "    #depends on the dimmension of B\n",
    "    \n",
    "    B0 = B[0,0]\n",
    "    B1 = B[0,1]\n",
    "    \n",
    "    df0b0 = -2* (np.exp(B0+B1*x)*y - 2*np.exp(2*B1*x+2*B0)) # df0 / dB_0\n",
    "    df0b1 = -2* (np.exp(B0+B1*x)*x*y - 2*np.exp(2*B1*x+2*B0)*x) # df0 / dB_1\n",
    "    df1b0 = -2*x* (np.exp(B1*x+B0)*y - 2*np.exp(2*B1*x+2*B0)) # df1 / dB_0\n",
    "    df1b1 = -2*x* (np.exp(B1*x+B0)*x*y - 2*np.exp(2*B1*x+2*B0)*x) # df1 / dB_1\n",
    "    \n",
    "    return np.array([\n",
    "            [np.sum(df0b0), np.sum(df0b1)], \n",
    "            [np.sum(df1b0), np.sum(df1b1)]\n",
    "            ])\n",
    "\n",
    "#Gradient Descent\n",
    "\"\"\"\n",
    "Loss Function = R(B) = SUM{ R_i(B) } = SUM{ (y_i - yhat_i(B))^2 }\n",
    "\n",
    "B(r+1) = B(r) - eta * Grad( Loss Function )\n",
    "B(r+1) = B(r) - eta * SUM{ Grad{ (y_i - yhat_i(B))^2  }}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, B, learning_rate, max_iter):\n",
    "    \n",
    "    loss_history = []\n",
    "    iteration = 0\n",
    "    B_history = [B]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        B = B.reshape(1,-1)\n",
    "    \n",
    "        B0 = B[0,0] \n",
    "        B1 = B[0,1]\n",
    "        B2 = B[0,2]\n",
    "        B3 = B[0,3]\n",
    "\n",
    "        y_hat = B0*np.sin(B1*x-B2)+B3*x  # depends on the model function\n",
    "        loss = np.sum((y-y_hat)**2)\n",
    "        \n",
    "        #stop if there are no improvements in the loss\n",
    "        if iteration > 0:\n",
    "            if np.abs(loss - loss_history[0]) == 0:\n",
    "                iteration = iteration - 1\n",
    "                break\n",
    "                \n",
    "        #keep track of the loss for each iteration)\n",
    "        loss_history.insert(0,loss)\n",
    "        \n",
    "        gradient = calc_gradient(B,y,x)\n",
    "        \n",
    "        diff = learning_rate * gradient\n",
    "\n",
    "        B = B-diff\n",
    "        B_history.append(B)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        \n",
    "    return loss, learning_rate, B, iteration, loss_history\n",
    "\n",
    "#Newton Raphson\n",
    "\"\"\"\n",
    "Loss Function = R(B) = SUM{ R_i(B) } = SUM{ (y_i - yhat_i(B))^2 }\n",
    "\n",
    "B(r+1) = B(r) - Jac( Loss Function )^-1 * Grad( Loss Function )\n",
    "B(r+1) = B(r) - Jac( (y_i - yhat_i(B))^2 )^-1 *  Grad( (y_i - yhat_i(B))^2 )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def newton_raphson(x, y, B, max_iter):\n",
    "    \n",
    "    loss_history = []\n",
    "    iteration = 0\n",
    "    B_history = [B]\n",
    "\n",
    "        \n",
    "    for i in range(max_iter):        \n",
    "        y_hat = np.exp(B[0,0] + B[0,1]*x)\n",
    "        loss = np.sum((y - y_hat)**2)\n",
    "                            \n",
    "        #stop if there are no improvements in the loss\n",
    "        if iteration > 0:\n",
    "            if np.abs(loss - loss_history[0]) == 0:\n",
    "                iteration = iteration - 1\n",
    "                break   \n",
    "        \n",
    "        #keep track of the loss for each iteration\n",
    "        loss_history.insert(0,loss)\n",
    "                \n",
    "        gradient = calc_gradient(B,y,x)\n",
    "        Jacobian = calc_Jacobian(B,y,x)\n",
    "        diff = np.matmul(np.linalg.inv(Jacobian),gradient)\n",
    "        \n",
    "        B = B - diff\n",
    "        B_history.append(B)\n",
    "\n",
    "        iteration = iteration + 1       \n",
    "        \n",
    "        \n",
    "                \n",
    "    return loss, B, iteration, loss_history, B_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0434b05",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b6679a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241m.\u001b[39mloc[:, df1\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "x = df1.loc[:, df1.columns != 'SalePrice']\n",
    "y = df1['SalePrice']\n",
    "\n",
    "model.fit(x, y)\n",
    "\n",
    "y_hat = model.predict(x)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model_train_rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "#score = model.score(x, y)\n",
    "print(model_train_rmse)\n",
    "model_cv_rmse = cross_validation(df1,LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a42eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds_3_10",
   "language": "python",
   "name": "env_ds_3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
