{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338a7d00",
   "metadata": {},
   "source": [
    "## Midterm Cheat-Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5a29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d123918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data file\n",
    "df = pd.read_csv('data3.csv')\n",
    "#Filtering Data\n",
    "df = df[df['FACE']  >= 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e01b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box-Cox transformation\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method = 'box-cox', standardize = False)\n",
    "\n",
    "# reshaping the column FACE to be in the appropriate dimension for box-cox transformation\n",
    "FACEbc = np.asarray(df['FACE'])\n",
    "FACEbc = FACEbc.reshape(-1,1)\n",
    "\n",
    "# converting type of FACE to float instead of int\n",
    "# so that I can convert 0 to a very small number instead\n",
    "FACEbc = FACEbc.astype(float)\n",
    "FACEbc[FACEbc == 0] = 0.0000000001\n",
    "\n",
    "pt.fit(FACEbc)\n",
    "\n",
    "#Adding the transformed FACE values into a new column on dataframe\n",
    "FACEbc = pt.transform(FACEbc)\n",
    "df['FACEbc'] = FACEbc\n",
    "\n",
    "skew1 = round(df['FACE'].skew(),3)\n",
    "skew2 = round(df['FACEbc'].skew(),3)\n",
    "lambdas = pt.lambdas_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878ab17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Centered and Normalizing\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def center_normalize_df (dataframe):\n",
    "    \n",
    "    for column in dataframe:\n",
    "        dataframe[column] = dataframe[column] - np.mean(dataframe[column])\n",
    "        dataframe[column] = dataframe[column] / norm(dataframe[column])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def center_normalize_array (array):\n",
    "    array = array - np.mean(array)\n",
    "    array = array / norm(array)\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb2f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "\n",
    "#Ploting two axes in one plot\n",
    "#Plotting Histogram\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(df['FACE'])\n",
    "ax1.set_title('histogram: skew = %s' % skew1)\n",
    "ax1.set_xlabel('original')\n",
    "\n",
    "ax2.hist(df['FACEbc'])\n",
    "ax2.set_title('histogram: skew = %s' % skew2)\n",
    "ax2.set_xlabel('optimal transformation: %s = %s' % (chr(0x03BB), round(lambdas,3)))\n",
    "\n",
    "#plot\n",
    "plt.show()\n",
    "\n",
    "#save to file\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"histogram.png\")\n",
    "\n",
    "\n",
    "\n",
    "#plotting line plot\n",
    "a = np.arange(0,updates-1,1)\n",
    "b = np.flip(results[4])\n",
    "\n",
    "plt.plot(a,b)\n",
    "plt.title('loss')\n",
    "plt.xlabel('update number')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plotting two line with scatter\n",
    "y_truth = 2 * np.sin(0.5*x - 3) +  0.1*x\n",
    "y_fitted = B[0,0] * np.sin(B[0,1]*x - B[0,2]) +  B[0,3]*x\n",
    "\n",
    "plt.scatter(x,y, label = \"observations\", alpha=0.5)\n",
    "plt.plot(x,y_truth, label = \"truth\")\n",
    "plt.plot(x,y_fitted, label = \"fitted\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d7506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4869597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy Variables for categorical variables\n",
    "#creating dummy variables for the categorical variable MARSTAT \n",
    "df_dc = pd.get_dummies(df, columns=['MARSTAT'])\n",
    "df_dc.head()\n",
    "\n",
    "#Adding ln(INCOME) as a new column on the dataframe \n",
    "df_dc['logINCOME'] = np.log(df_dc['INCOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "#Computing linear regression\n",
    "#Formula FACEbc = B0 + B1*EDUCATION + B2*NUMHH + B3*logINCOME + B4*MARSTAT_0 + B5*MARSTAT_2 + E\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = df_dc[['EDUCATION', 'NUMHH', 'logINCOME', 'MARSTAT_0', 'MARSTAT_2']]\n",
    "y = df_dc['FACEbc']\n",
    "reg = LinearRegression().fit(X, y)\n",
    "B = reg.coef_\n",
    "B = np.insert(B, 0, reg.intercept_, axis=0)\n",
    "print(B)\n",
    "\n",
    "#Computing standard error with n-1 degrees of freedom\n",
    "SE = np.std(y, ddof=1)\n",
    "print(SE)\n",
    "\n",
    "#Computing R2\n",
    "y_pred = reg.predict(X)\n",
    "from sklearn.metrics import r2_score\n",
    "R2 = r2_score(y, y_pred)\n",
    "print(R2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Conducting linear regression using OLS method\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = df_dc[['EDUCATION', 'NUMHH', 'logINCOME', 'MARSTAT_0', 'MARSTAT_2']]\n",
    "y = df_dc['FACEbc']\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "lm = sm.OLS(y, X2).fit()\n",
    "print(lm.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Prediction\n",
    "\n",
    "#Conducting prediction with the married man profile as described from the question\n",
    "X3 = np.array([['16', '4', np.log(120000), '0', '0']])\n",
    "Y3 = reg.predict(X3)\n",
    "\n",
    "#Inversing the box-cox transformation for the prediction\n",
    "Y3 = Y3.reshape(-1,1)\n",
    "Y3 = pt.inverse_transform(Y3)\n",
    "\n",
    "#Inversing the box-cox transformation of the standard error\n",
    "SE = SE.reshape(-1,1)\n",
    "SE = pt.inverse_transform(SE)\n",
    "\n",
    "#Computing upper and lower interval\n",
    "Y3_upper, Y3_lower = Y3+SE, Y3-SE\n",
    "\n",
    "print('95%% likelihood that the true value is between $%.2f and $%.2f' % (Y3_lower, Y3_upper))\n",
    "print('True value: $%.2f' % Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45426f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table\n",
    "iteration2 = np.arange(0,iteration+2, 1)\n",
    "df2 = pd.DataFrame(list(zip(iteration2, B_history, np.flip(loss_history))),\n",
    "                   columns = ['iteration', 'B_hat', 'loss'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "RMSE = np.sqrt(((y_hat - y) ** 2).mean())\n",
    "print('The estimated RMSE via LOOCV is\\nRMSE = %s' % (RMSE))\n",
    "\n",
    "#RMSE using sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y, y_hat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e656282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 Fold CV\n",
    "x = df['x']\n",
    "y = df['y']\n",
    "B = np.array([0.5, 0.75])\n",
    "B = B.reshape(1,-1)\n",
    "kf_df = pd.DataFrame(zip(x,y), columns = ['x', 'y'])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "kf_rmse = []\n",
    "\n",
    "for train, test in kf.split(kf_df):\n",
    "    X_train, X_test = x[train], x[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    nr = newton_raphson(X_train,y_train,B,10000)\n",
    "    y_hat = np.exp(nr[1][0,1] + nr[1][0,1]*X_test)\n",
    "    kf_rmse.append(mean_squared_error(y_test, y_hat, squared=False))\n",
    "\n",
    "kf_RMSE = (1/10) * np.sum(kf_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c170142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(B, y, x):\n",
    "    #depends on the dimmension of B\n",
    "    \n",
    "    B0 = B[0,0] \n",
    "    B1 = B[0,1]\n",
    "    \n",
    "    f0 = -2*np.exp(B0+B1*x)*(y-np.exp(B0+B1*x)) # dR/dB_0\n",
    "    f1 = -2*np.exp(B1*x+B0)*x*(y-np.exp(B0+B1*x)) # dR/dB_1\n",
    "    \n",
    "    return np.array([np.sum(f0), np.sum(f1)])\n",
    "\n",
    "def calc_Jacobian(B, y, x):\n",
    "    #depends on the dimmension of B\n",
    "    \n",
    "    B0 = B[0,0]\n",
    "    B1 = B[0,1]\n",
    "    \n",
    "    df0b0 = -2* (np.exp(B0+B1*x)*y - 2*np.exp(2*B1*x+2*B0)) # df0 / dB_0\n",
    "    df0b1 = -2* (np.exp(B0+B1*x)*x*y - 2*np.exp(2*B1*x+2*B0)*x) # df0 / dB_1\n",
    "    df1b0 = -2*x* (np.exp(B1*x+B0)*y - 2*np.exp(2*B1*x+2*B0)) # df1 / dB_0\n",
    "    df1b1 = -2*x* (np.exp(B1*x+B0)*x*y - 2*np.exp(2*B1*x+2*B0)*x) # df1 / dB_1\n",
    "    \n",
    "    return np.array([\n",
    "            [np.sum(df0b0), np.sum(df0b1)], \n",
    "            [np.sum(df1b0), np.sum(df1b1)]\n",
    "            ])\n",
    "\n",
    "#Gradient Descent\n",
    "\"\"\"\n",
    "Loss Function = R(B) = SUM{ R_i(B) } = SUM{ (y_i - yhat_i(B))^2 }\n",
    "\n",
    "B(r+1) = B(r) - eta * Grad( Loss Function )\n",
    "B(r+1) = B(r) - eta * SUM{ Grad{ (y_i - yhat_i(B))^2  }}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, B, learning_rate, max_iter):\n",
    "    \n",
    "    loss_history = []\n",
    "    iteration = 0\n",
    "    B_history = [B]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        B = B.reshape(1,-1)\n",
    "    \n",
    "        B0 = B[0,0] \n",
    "        B1 = B[0,1]\n",
    "        B2 = B[0,2]\n",
    "        B3 = B[0,3]\n",
    "\n",
    "        y_hat = B0*np.sin(B1*x-B2)+B3*x  # depends on the model function\n",
    "        loss = np.sum((y-y_hat)**2)\n",
    "        \n",
    "        #stop if there are no improvements in the loss\n",
    "        if iteration > 0:\n",
    "            if np.abs(loss - loss_history[0]) == 0:\n",
    "                iteration = iteration - 1\n",
    "                break\n",
    "                \n",
    "        #keep track of the loss for each iteration)\n",
    "        loss_history.insert(0,loss)\n",
    "        \n",
    "        gradient = calc_gradient(B,y,x)\n",
    "        \n",
    "        diff = learning_rate * gradient\n",
    "\n",
    "        B = B-diff\n",
    "        B_history.append(B)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        \n",
    "    return loss, learning_rate, B, iteration, loss_history\n",
    "\n",
    "#Newton Raphson\n",
    "\"\"\"\n",
    "Loss Function = R(B) = SUM{ R_i(B) } = SUM{ (y_i - yhat_i(B))^2 }\n",
    "\n",
    "B(r+1) = B(r) - Jac( Loss Function )^-1 * Grad( Loss Function )\n",
    "B(r+1) = B(r) - Jac( (y_i - yhat_i(B))^2 )^-1 *  Grad( (y_i - yhat_i(B))^2 )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def newton_raphson(x, y, B, max_iter):\n",
    "    \n",
    "    loss_history = []\n",
    "    iteration = 0\n",
    "    B_history = [B]\n",
    "\n",
    "        \n",
    "    for i in range(max_iter):        \n",
    "        y_hat = np.exp(B[0,0] + B[0,1]*x)\n",
    "        loss = np.sum((y - y_hat)**2)\n",
    "                            \n",
    "        #stop if there are no improvements in the loss\n",
    "        if iteration > 0:\n",
    "            if np.abs(loss - loss_history[0]) == 0:\n",
    "                iteration = iteration - 1\n",
    "                break   \n",
    "        \n",
    "        #keep track of the loss for each iteration\n",
    "        loss_history.insert(0,loss)\n",
    "                \n",
    "        gradient = calc_gradient(B,y,x)\n",
    "        Jacobian = calc_Jacobian(B,y,x)\n",
    "        diff = np.matmul(np.linalg.inv(Jacobian),gradient)\n",
    "        \n",
    "        B = B - diff\n",
    "        B_history.append(B)\n",
    "\n",
    "        iteration = iteration + 1       \n",
    "        \n",
    "        \n",
    "                \n",
    "    return loss, B, iteration, loss_history, B_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540cf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds_3_10",
   "language": "python",
   "name": "env_ds_3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
